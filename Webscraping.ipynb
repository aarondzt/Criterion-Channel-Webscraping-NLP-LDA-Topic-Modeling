{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make soup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "request = requests.get('https://films.criterionchannel.com/')\n",
    "soup = BeautifulSoup(request.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape titles, get rid of tabs and new lines\n",
    "titles = []\n",
    "for title in soup.findAll(class_ = \"criterion-channel__td criterion-channel__td--title\"):\n",
    "    nt = title.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    titles.append(no_nt)\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape urls\n",
    "urls = []\n",
    "for url in soup.findAll('a', href = True):\n",
    "    urls.append(url.get('href'))\n",
    "# Only keep urls that correspond to films\n",
    "urls = urls[3:]\n",
    "urls = urls[1:-21]\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape directors\n",
    "directors = []\n",
    "for director in soup.findAll(class_ = 'criterion-channel__td criterion-channel__td--director'):\n",
    "    nt = director.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    directors.append(no_nt)\n",
    "print(len(directors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape countries\n",
    "countries = []\n",
    "for country in soup.findAll(class_ = 'criterion-channel__td criterion-channel__td--country'):\n",
    "    nt = country.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    no_comma = no_nt[:-1]\n",
    "    countries.append(no_comma)\n",
    "print(len(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape years\n",
    "years = []\n",
    "for year in soup.findAll(class_ = 'criterion-channel__td criterion-channel__td--year'):\n",
    "    nt = year.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    years.append(no_nt)\n",
    "print(len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({'Title': titles, 'Director': directors, 'Country': countries, 'Year': years, 'Url': urls})\n",
    "# Remove rows without durations (parts > 1 of a film)\n",
    "data = data[~data['Url'].str.contains('/videos/')]\n",
    "# Remove two rows with urls that don't work\n",
    "# ....\n",
    "data = data.reset_index(drop = True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check for broken links, do not run this, it takes a long time\n",
    "# fourohfour = []\n",
    "# for url in data['Url']:\n",
    "#     # 200 = working, 404 = broken\n",
    "#     fourohfour.append(requests.get(url))\n",
    "#     print(url)\n",
    "# print(len(fourohfour))\n",
    "# # Save as text file (Excel often incorrectly reformats csv files upon opening)\n",
    "# with open('data/Fourohfour.txt', 'w') as file:\n",
    "#     for line in fourohfour:\n",
    "#         file.write(\"%s\\n\" % line)\n",
    "# print(len(fourohfour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pre-scraped 404 file\n",
    "with open('data\\Fourohfour.txt') as file:\n",
    "    fourohfour = file.read().splitlines()\n",
    "# Insert 404 column\n",
    "data.insert(5, '404', fourohfour)\n",
    "# Convert from BeautifulSoup type to string\n",
    "data['404'] = data['404'].astype(str)\n",
    "# Remove 404 rows from data\n",
    "data = data[~data['404'].str.contains('404')]\n",
    "print(len(data)) # Removed 52 broken links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after filtering out rows\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Scrape durations, do not run this, it takes a long time\n",
    "# durations = []\n",
    "# for url in data['Url']:\n",
    "#     request = requests.get(url)\n",
    "#     soup = BeautifulSoup(request.content, 'html.parser')\n",
    "#     for duration in soup.findAll(class_ = 'duration-container')[:1]:\n",
    "#         durations.append(duration.get_text())\n",
    "#     print(url)\n",
    "# # Save as text file\n",
    "# with open('data/Durations.txt', 'w') as file:\n",
    "#     for line in durations:\n",
    "#         file.write(\"%s\\n\" % line)\n",
    "# print(len(durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pre-scraped duration file\n",
    "with open('data\\Durations.txt') as file:\n",
    "    durations = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean durations\n",
    "durations = durations[1:]\n",
    "durations = durations[::3]\n",
    "durations = [x.strip(' ') for x in durations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert duration column\n",
    "try:\n",
    "    data.insert(4, 'Duration', durations)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seconds, keep only hours and minutes\n",
    "data['Duration'] = data['Duration'].str[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append '0:' to beginning of duration to indicate 0 hours for all films < 1 hour\n",
    "# that are not formatted consistently with the rest of the data\n",
    "for i, duration in enumerate(data['Duration']):\n",
    "    if ':' not in duration:\n",
    "        data.loc[i, 'Duration'] = '0:' + duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split duration by colon\n",
    "hours_minutes = data['Duration'].str.split(':', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert hours and minutes columns\n",
    "data.insert(5, 'Hours', hours_minutes[0])\n",
    "data['Hours'] = data['Hours'].astype(int)\n",
    "data.insert(6, 'Minutes', hours_minutes[1])\n",
    "data['Minutes'] = data['Minutes'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and insert total hours\n",
    "total_hours = []\n",
    "for i in range(len(data)):\n",
    "    hours = (data.loc[i]['Hours'].astype(int) + data.loc[i]['Minutes'].astype(int)/60).round(2)\n",
    "    total_hours.append(hours)\n",
    "try:\n",
    "    data.insert(7, 'Total Hours', total_hours)\n",
    "except:\n",
    "    pass\n",
    "# Drop old columns\n",
    "try:\n",
    "    data = data.drop(['Minutes', 'Hours', '404'], axis = 1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Scrape descriptions, do not run this, it takes a long time\n",
    "# descriptions = []\n",
    "# for url in data['Url']:\n",
    "#     request = requests.get(url)\n",
    "#     soup = BeautifulSoup(request.content, 'html.parser')\n",
    "#     paragraphs = soup.findAll('p')\n",
    "#     # Select paragraph containing the description\n",
    "#     paragraphs = paragraphs[1]\n",
    "#     string = []\n",
    "#     for x in paragraphs:\n",
    "#         string.append(str(x))\n",
    "#     descriptions.append(string[0])\n",
    "#     print(url)\n",
    "# # Save to csv (list is incorrectly loaded as text file)\n",
    "# descriptions = pd.DataFrame({'Description': descriptions})\n",
    "# descriptions.to_csv('data/Descriptions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pre-scraped description file\n",
    "descriptions = pd.read_csv('data\\Descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert description column\n",
    "data.insert(5, 'Description', descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove films < 1 hour, as these are mostly shorts, not films\n",
    "data = data[data['Total Hours'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decade column\n",
    "import numpy as np\n",
    "try:\n",
    "    data.insert(4, 'Decade', (data['Year'].astype(int)/10).apply(np.floor))\n",
    "except:\n",
    "    pass\n",
    "data['Decade'] = data['Decade'].astype(str)\n",
    "data['Decade'] = data['Decade'].str.replace('.', '')\n",
    "data['Decade'] = data['Decade'].astype(str) + 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 'None'\n",
    "data = data.replace(np.nan, 'None', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "data.to_csv('data\\Criterion.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
