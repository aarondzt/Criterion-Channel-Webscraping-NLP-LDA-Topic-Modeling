{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "For this project, I decided to use the Criterion Channel Collection for a webscraping exercise. The Criterion Channel is a streaming service like Netflix, but for the classic films inducted into the Criterion Collection. I am interested in what directors, countries, and decades are most highly represented in this collection.\n",
    "## Scraping policy\n",
    "As far as I could tell, the Criterion Channel has no scraping policy. I encountered no obstacles in scraping the entire contents of their collection multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Scraping the main page\n",
    "In this section, I ran for loops over the main page of the collection to scrape titles, urls, directors, countries and years of release. I filtered out multi-part films that do not contain this data, incorrectly formatted films, and films with broken links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make soup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "request = requests.get('https://films.criterionchannel.com/')\n",
    "soup = BeautifulSoup(request.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "# Scrape titles, get rid of tabs and new lines\n",
    "titles = []\n",
    "for title in soup.findAll(class_ = \"criterion-channel__td criterion-channel__td--title\"):\n",
    "    nt = title.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    titles.append(no_nt)\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "# Scrape urls\n",
    "urls = []\n",
    "for url in soup.findAll('a', href = True):\n",
    "    urls.append(url.get('href'))\n",
    "# Only keep urls that correspond to films\n",
    "urls = urls[3:]\n",
    "urls = urls[1:-21]\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "# Scrape directors\n",
    "directors = []\n",
    "for director in soup.findAll(class_ = 'criterion-channel__td criterion-channel__td--director'):\n",
    "    nt = director.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    directors.append(no_nt)\n",
    "print(len(directors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "# Scrape countries\n",
    "countries = []\n",
    "for country in soup.findAll(class_ = 'criterion-channel__td criterion-channel__td--country'):\n",
    "    nt = country.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    no_comma = no_nt[:-1]\n",
    "    countries.append(no_comma)\n",
    "print(len(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "# Scrape years\n",
    "years = []\n",
    "for year in soup.findAll(class_ = 'criterion-channel__td criterion-channel__td--year'):\n",
    "    nt = year.get_text()\n",
    "    no_t = nt.replace('\\t', '')\n",
    "    no_nt = no_t.replace('\\n', '')\n",
    "    years.append(no_nt)\n",
    "print(len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({'Title': titles, 'Director': directors, 'Country': countries, 'Year': years, 'Url': urls})\n",
    "# Remove rows without durations (parts > 1 of a film)\n",
    "data = data[~data['Url'].str.contains('/videos/')]\n",
    "# Remove two rows with urls that don't work\n",
    "# ....\n",
    "data = data.reset_index(drop = True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check for broken links, do not run this, it takes a long time\n",
    "# fourohfour = []\n",
    "# for url in data['Url']:\n",
    "#     # 200 = working, 404 = broken\n",
    "#     fourohfour.append(requests.get(url))\n",
    "#     print(url)\n",
    "# print(len(fourohfour))\n",
    "# # Save as text file (Excel often incorrectly reformats csv files upon opening)\n",
    "# with open('data/Fourohfour.txt', 'w') as file:\n",
    "#     for line in fourohfour:\n",
    "#         file.write(\"%s\\n\" % line)\n",
    "# print(len(fourohfour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n"
     ]
    }
   ],
   "source": [
    "# Open pre-scraped 404 file\n",
    "with open('data\\Fourohfour.txt') as file:\n",
    "    fourohfour = file.read().splitlines()\n",
    "# Insert 404 column\n",
    "data.insert(5, '404', fourohfour)\n",
    "# Convert from BeautifulSoup type to string\n",
    "data['404'] = data['404'].astype(str)\n",
    "# Remove 404 rows from data\n",
    "data = data[~data['404'].str.contains('404')]\n",
    "print(len(data)) # Removed 52 broken links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after filtering out rows\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Director</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Url</th>\n",
       "      <th>404</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 or 3 Things I Know About Her</td>\n",
       "      <td>Jean-Luc Godard</td>\n",
       "      <td>France</td>\n",
       "      <td>1967</td>\n",
       "      <td>https://www.criterionchannel.com/2-or-3-things...</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les 3 boutons</td>\n",
       "      <td>Agnès Varda</td>\n",
       "      <td>France</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://www.criterionchannel.com/les-3-boutons</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Faces</td>\n",
       "      <td>Jafar Panahi</td>\n",
       "      <td>Iran</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://www.criterionchannel.com/3-faces</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 Months, 3 Weeks and 2 Days</td>\n",
       "      <td>Cristian Mungiu</td>\n",
       "      <td>Romania</td>\n",
       "      <td>2007</td>\n",
       "      <td>https://www.criterionchannel.com/4-months-3-we...</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 Quarters</td>\n",
       "      <td>Ashley McKenzie</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://www.criterionchannel.com/4-quarters</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Title         Director  Country  Year  \\\n",
       "0  2 or 3 Things I Know About Her  Jean-Luc Godard   France  1967   \n",
       "1                   Les 3 boutons      Agnès Varda   France  2015   \n",
       "2                         3 Faces     Jafar Panahi     Iran  2018   \n",
       "3    4 Months, 3 Weeks and 2 Days  Cristian Mungiu  Romania  2007   \n",
       "4                      4 Quarters  Ashley McKenzie   Canada  2015   \n",
       "\n",
       "                                                 Url               404  \n",
       "0  https://www.criterionchannel.com/2-or-3-things...  <Response [200]>  \n",
       "1     https://www.criterionchannel.com/les-3-boutons  <Response [200]>  \n",
       "2           https://www.criterionchannel.com/3-faces  <Response [200]>  \n",
       "3  https://www.criterionchannel.com/4-months-3-we...  <Response [200]>  \n",
       "4        https://www.criterionchannel.com/4-quarters  <Response [200]>  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Scraping within each film's own page\n",
    "In order to obtain the duration and description of the films, I needed to write a for loop that entered into each film's page via its url. Because this takes a long time (> 1000 films), I saved the results into a .txt or .csv file so I would not have to re-run the scraping each time I tested out the code. I wanted to focus on feature length films for my analysis, so I wrote some code to conver the HH:MM:SS string-type duration data into a \"Total Hours\" float-type, and then excluded all films < 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Scrape durations, do not run this, it takes a long time\n",
    "# durations = []\n",
    "# for url in data['Url']:\n",
    "#     request = requests.get(url)\n",
    "#     soup = BeautifulSoup(request.content, 'html.parser')\n",
    "#     for duration in soup.findAll(class_ = 'duration-container')[:1]:\n",
    "#         durations.append(duration.get_text())\n",
    "#     print(url)\n",
    "# # Save as text file\n",
    "# with open('data/Durations.txt', 'w') as file:\n",
    "#     for line in durations:\n",
    "#         file.write(\"%s\\n\" % line)\n",
    "# print(len(durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pre-scraped duration file\n",
    "with open('data\\Durations.txt') as file:\n",
    "    durations = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean durations\n",
    "durations = durations[1:]\n",
    "durations = durations[::3]\n",
    "durations = [x.strip(' ') for x in durations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert duration column\n",
    "try:\n",
    "    data.insert(4, 'Duration', durations)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seconds, keep only hours and minutes\n",
    "data['Duration'] = data['Duration'].str[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append '0:' to beginning of duration to indicate 0 hours for all films < 1 hour\n",
    "# that are not formatted consistently with the rest of the data\n",
    "for i, duration in enumerate(data['Duration']):\n",
    "    if ':' not in duration:\n",
    "        data.loc[i, 'Duration'] = '0:' + duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split duration by colon\n",
    "hours_minutes = data['Duration'].str.split(':', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert hours and minutes columns\n",
    "data.insert(5, 'Hours', hours_minutes[0])\n",
    "data['Hours'] = data['Hours'].astype(int)\n",
    "data.insert(6, 'Minutes', hours_minutes[1])\n",
    "data['Minutes'] = data['Minutes'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and insert total hours\n",
    "total_hours = []\n",
    "for i in range(len(data)):\n",
    "    hours = (data.loc[i]['Hours'].astype(int) + data.loc[i]['Minutes'].astype(int)/60).round(2)\n",
    "    total_hours.append(hours)\n",
    "try:\n",
    "    data.insert(7, 'Total Hours', total_hours)\n",
    "except:\n",
    "    pass\n",
    "# Drop old columns\n",
    "try:\n",
    "    data = data.drop(['Minutes', 'Hours', '404'], axis = 1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Scrape descriptions, do not run this, it takes a long time\n",
    "# descriptions = []\n",
    "# for url in data['Url']:\n",
    "#     request = requests.get(url)\n",
    "#     soup = BeautifulSoup(request.content, 'html.parser')\n",
    "#     paragraphs = soup.findAll('p')\n",
    "#     # Select paragraph containing the description\n",
    "#     paragraphs = paragraphs[1]\n",
    "#     string = []\n",
    "#     for x in paragraphs:\n",
    "#         string.append(str(x))\n",
    "#     descriptions.append(string[0])\n",
    "#     print(url)\n",
    "# # Save to csv (list is incorrectly loaded as text file)\n",
    "# descriptions = pd.DataFrame({'Description': descriptions})\n",
    "# descriptions.to_csv('data/Descriptions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pre-scraped description file\n",
    "descriptions = pd.read_csv('data\\Descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert description column\n",
    "data.insert(5, 'Description', descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove films < 1 hour, as these are mostly shorts, not films\n",
    "data = data[data['Total Hours'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-95e417f1eb5c>:8: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  data['Decade'] = data['Decade'].str.replace('.', '')\n"
     ]
    }
   ],
   "source": [
    "# Create decade column\n",
    "import numpy as np\n",
    "try:\n",
    "    data.insert(4, 'Decade', (data['Year'].astype(int)/10).apply(np.floor))\n",
    "except:\n",
    "    pass\n",
    "data['Decade'] = data['Decade'].astype(str)\n",
    "data['Decade'] = data['Decade'].str.replace('.', '')\n",
    "data['Decade'] = data['Decade'].astype(str) + 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 'None'\n",
    "data = data.replace(np.nan, 'None', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "data.to_csv('data\\Criterion.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "data = pd.read_csv('data\\Criterion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Director</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Decade</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Description</th>\n",
       "      <th>Total Hours</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 or 3 Things I Know About Her</td>\n",
       "      <td>Jean-Luc Godard</td>\n",
       "      <td>France</td>\n",
       "      <td>1967</td>\n",
       "      <td>1960s</td>\n",
       "      <td>1:27</td>\n",
       "      <td>In 2 OR 3 THINGS I KNOW ABOUT HER (2 OU 3 CHOS...</td>\n",
       "      <td>1.45</td>\n",
       "      <td>https://www.criterionchannel.com/2-or-3-things...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 Faces</td>\n",
       "      <td>Jafar Panahi</td>\n",
       "      <td>Iran</td>\n",
       "      <td>2018</td>\n",
       "      <td>2010s</td>\n",
       "      <td>1:40</td>\n",
       "      <td>Iranian master Jafar Panahi’s fourth feature s...</td>\n",
       "      <td>1.67</td>\n",
       "      <td>https://www.criterionchannel.com/3-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 Months, 3 Weeks and 2 Days</td>\n",
       "      <td>Cristian Mungiu</td>\n",
       "      <td>Romania</td>\n",
       "      <td>2007</td>\n",
       "      <td>2000s</td>\n",
       "      <td>1:53</td>\n",
       "      <td>Romanian filmmaker Cristian Mungiu shot to int...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>https://www.criterionchannel.com/4-months-3-we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The VI Olympic Winter Games, Oslo 1952</td>\n",
       "      <td>Tankred Ibsen</td>\n",
       "      <td>Norway</td>\n",
       "      <td>1952</td>\n",
       "      <td>1950s</td>\n",
       "      <td>1:43</td>\n",
       "      <td>Director Tancred Ibsen's penchant for depictin...</td>\n",
       "      <td>1.72</td>\n",
       "      <td>https://www.criterionchannel.com/the-vi-olympi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8½</td>\n",
       "      <td>Federico Fellini</td>\n",
       "      <td>Italy</td>\n",
       "      <td>1963</td>\n",
       "      <td>1960s</td>\n",
       "      <td>2:19</td>\n",
       "      <td>Marcello Mastroianni plays Guido Anselmi, a di...</td>\n",
       "      <td>2.32</td>\n",
       "      <td>https://www.criterionchannel.com/81-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The IX Olympiad at Amsterdam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>1928</td>\n",
       "      <td>1920s</td>\n",
       "      <td>4:11</td>\n",
       "      <td>Made by Istituto Luce, the Italian film compan...</td>\n",
       "      <td>4.18</td>\n",
       "      <td>https://www.criterionchannel.com/the-ix-olympi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IX Olympic Winter Games, Innsbruck 1964</td>\n",
       "      <td>Theo Hörmann</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1964</td>\n",
       "      <td>1960s</td>\n",
       "      <td>1:30</td>\n",
       "      <td>Joy and good humor pervades Theo Hörmann's doc...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>https://www.criterionchannel.com/ix-olympic-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13 Days in France</td>\n",
       "      <td>François Reichenbach…</td>\n",
       "      <td>France</td>\n",
       "      <td>1968</td>\n",
       "      <td>1960s</td>\n",
       "      <td>1:52</td>\n",
       "      <td>13 DAYS IN FRANCE, a personal project for Fren...</td>\n",
       "      <td>1.87</td>\n",
       "      <td>https://www.criterionchannel.com/13-days-in-fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XIVth Olympiad: The Glory of Sport</td>\n",
       "      <td>Castleton Knight</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1948</td>\n",
       "      <td>1940s</td>\n",
       "      <td>2:18</td>\n",
       "      <td>The official film of the Games of the XIV Olym...</td>\n",
       "      <td>2.30</td>\n",
       "      <td>https://www.criterionchannel.com/xivth-olympia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16 Days of Glory</td>\n",
       "      <td>Bud Greenspan</td>\n",
       "      <td>United States</td>\n",
       "      <td>1986</td>\n",
       "      <td>1980s</td>\n",
       "      <td>4:44</td>\n",
       "      <td>Director Bud Greenspan, whose career covering ...</td>\n",
       "      <td>4.73</td>\n",
       "      <td>https://www.criterionchannel.com/16-days-of-glory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title               Director  \\\n",
       "0           2 or 3 Things I Know About Her        Jean-Luc Godard   \n",
       "1                                  3 Faces           Jafar Panahi   \n",
       "2             4 Months, 3 Weeks and 2 Days        Cristian Mungiu   \n",
       "3   The VI Olympic Winter Games, Oslo 1952          Tankred Ibsen   \n",
       "4                                       8½       Federico Fellini   \n",
       "5             The IX Olympiad at Amsterdam                    NaN   \n",
       "6  IX Olympic Winter Games, Innsbruck 1964           Theo Hörmann   \n",
       "7                        13 Days in France  François Reichenbach…   \n",
       "8       XIVth Olympiad: The Glory of Sport       Castleton Knight   \n",
       "9                         16 Days of Glory          Bud Greenspan   \n",
       "\n",
       "          Country  Year Decade Duration  \\\n",
       "0          France  1967  1960s     1:27   \n",
       "1            Iran  2018  2010s     1:40   \n",
       "2         Romania  2007  2000s     1:53   \n",
       "3          Norway  1952  1950s     1:43   \n",
       "4           Italy  1963  1960s     2:19   \n",
       "5     Netherlands  1928  1920s     4:11   \n",
       "6         Austria  1964  1960s     1:30   \n",
       "7          France  1968  1960s     1:52   \n",
       "8  United Kingdom  1948  1940s     2:18   \n",
       "9   United States  1986  1980s     4:44   \n",
       "\n",
       "                                         Description  Total Hours  \\\n",
       "0  In 2 OR 3 THINGS I KNOW ABOUT HER (2 OU 3 CHOS...         1.45   \n",
       "1  Iranian master Jafar Panahi’s fourth feature s...         1.67   \n",
       "2  Romanian filmmaker Cristian Mungiu shot to int...         1.88   \n",
       "3  Director Tancred Ibsen's penchant for depictin...         1.72   \n",
       "4  Marcello Mastroianni plays Guido Anselmi, a di...         2.32   \n",
       "5  Made by Istituto Luce, the Italian film compan...         4.18   \n",
       "6  Joy and good humor pervades Theo Hörmann's doc...         1.50   \n",
       "7  13 DAYS IN FRANCE, a personal project for Fren...         1.87   \n",
       "8  The official film of the Games of the XIV Olym...         2.30   \n",
       "9  Director Bud Greenspan, whose career covering ...         4.73   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.criterionchannel.com/2-or-3-things...  \n",
       "1           https://www.criterionchannel.com/3-faces  \n",
       "2  https://www.criterionchannel.com/4-months-3-we...  \n",
       "3  https://www.criterionchannel.com/the-vi-olympi...  \n",
       "4              https://www.criterionchannel.com/81-2  \n",
       "5  https://www.criterionchannel.com/the-ix-olympi...  \n",
       "6  https://www.criterionchannel.com/ix-olympic-wi...  \n",
       "7  https://www.criterionchannel.com/13-days-in-fr...  \n",
       "8  https://www.criterionchannel.com/xivth-olympia...  \n",
       "9  https://www.criterionchannel.com/16-days-of-glory  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Description 2'] = data['Director'] + ' - ' + data['Country'] + ' - ' + data['Decade'] + ' - ' + data['Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "f = open('C:/Users/HP/Documents/NLP/MySQL_stopwords.txt', 'r', encoding = 'utf-8')\n",
    "stop_words = f.read()\n",
    "stop_words = re.split(' \\t|\\n', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "# stop_words = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def normalize_corpora(corpora):\n",
    "    normalized_corpora = []\n",
    "    for i, corpus in enumerate(corpora):\n",
    "        # Lowercase\n",
    "        corpus = corpus.lower()\n",
    "        # Replace \n",
    "        corpus = corpus.replace(\"/\", \" \")\n",
    "        corpus = corpus.replace(\"’\", \"'\")\n",
    "        corpus = corpus.replace(\"'s\", \"\")\n",
    "        # Remove numbers\n",
    "        corpus = re.sub('[^A-Za-z0-9\\']+', ' ', corpus)\n",
    "        # Strip spaces\n",
    "        corpus_tokens = tokenizer.tokenize(corpus)\n",
    "        # Remove stopwords\n",
    "        corpus_tokens = [token for token in corpus_tokens if token not in stop_words]\n",
    "        # Lemmatize\n",
    "        corpus_tokens = [lemmatizer.lemmatize(token) for token in corpus_tokens if not token.isnumeric()]\n",
    "        # Remove single characters\n",
    "        corpus_tokens = [token for token in corpus_tokens if len(token) > 1]\n",
    "        # Remove empty corpus\n",
    "        if corpus_tokens:\n",
    "            normalized_corpora.append(corpus_tokens)\n",
    "    return normalized_corpora\n",
    "normalized_documents = normalize_corpora(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "bigram = gensim.models.Phrases(normalized_documents, min_count = 5, threshold = 5, delimiter = b'_')\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 15467\n"
     ]
    }
   ],
   "source": [
    "normalized_corpus_bigrams = [bigram_model[post] for post in normalized_documents]\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = gensim.corpora.Dictionary(normalized_corpus_bigrams)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 3277\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.5)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in normalized_corpus_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALLET_PATH = 'C:/mallet-2.0.8/bin/mallet'\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from tqdm import tqdm\n",
    "os.environ['MALLET_HOME'] = 'C:/mallet-2.0.8'\n",
    "\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary, \n",
    "                                    start_topic_count = 1, end_topic_count = 20, step = 1,\n",
    "                                    cpus = 8):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count + 1, step)):\n",
    "        mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path = MALLET_PATH, corpus = corpus,\n",
    "                                                            num_topics = topic_nums, id2word = dictionary,\n",
    "                                                            iterations = 100, workers = cpus, random_seed = 20210224)\n",
    "        cv_coherence_model_mallet_lda = gensim.models.CoherenceModel(model = mallet_lda_model, corpus = corpus, \n",
    "                                                                     texts = texts, dictionary = dictionary, \n",
    "                                                                     coherence = 'c_v')\n",
    "        coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(mallet_lda_model)\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                 | 1/40 [00:23<15:00, 23.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-21a01218eb82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mend_topic_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m lda_models, coherence_scores = topic_model_coherence_generator(corpus = bow_corpus, texts = normalized_corpus_bigrams,\n\u001b[0m\u001b[0;32m      3\u001b[0m                                                                \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_topic_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                                end_topic_count = end_topic_count, step = 1, cpus = 8)\n",
      "\u001b[1;32m<ipython-input-40-ac67ecba7835>\u001b[0m in \u001b[0;36mtopic_model_coherence_generator\u001b[1;34m(corpus, texts, dictionary, start_topic_count, end_topic_count, step, cpus)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcoherence_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic_nums\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_topic_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_topic_count\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path = MALLET_PATH, corpus = corpus,\n\u001b[0m\u001b[0;32m     14\u001b[0m                                                             \u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_nums\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                                             iterations = 100, workers = cpus, random_seed = 20210224)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;31m# NOTE \"--keep-sequence-bigrams\" / \"--use-ngrams true\" poorer results + runs out of memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training MALLET LDA with %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# NOTE - we are still keeping the wordtopics variable to not break backward compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m   1922\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"COMMAND: %s %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m         \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1013\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1015\u001b[1;33m                 \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1016\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "end_topic_count = 40\n",
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus = bow_corpus, texts = normalized_corpus_bigrams,\n",
    "                                                               dictionary = dictionary, start_topic_count = 1,\n",
    "                                                               end_topic_count = end_topic_count, step = 1, cpus = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(1, end_topic_count + 1, 1),\n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df = coherence_df.sort_values(by = 'Coherence Score', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(1, end_topic_count + 1, 1)\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c = 'r')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_idx = coherence_df['Number of Topics'].index[1]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in best_lda_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, best_lda_model.num_topics)]\n",
    "# for idx, topic in enumerate(topics):\n",
    "#     print('Topic #'+str(idx+1)+':')\n",
    "#     print([term for term, wt in topic])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in topics],\n",
    "                         columns = ['Topic Desc'],\n",
    "                         index = range(1, best_lda_model.num_topics + 1)\n",
    "                         )\n",
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_results = best_lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics = [sorted(topics, key = lambda record: -record[1])[0] for topics in tm_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(documents))\n",
    "corpus_topic_df['Dominant Topic'] = [item[0] + 1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1] * 100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Topic Desc'] for t in corpus_topics]\n",
    "corpus_topic_df['Post'] = documents\n",
    "corpus_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stats_df = corpus_topic_df.groupby('Dominant Topic').count()\n",
    "topic_stats_df = topic_stats_df.drop(['Contribution %', 'Topic Desc', 'Post'], axis = 1)\n",
    "topic_stats_df.columns = ['# of Docs']\n",
    "topic_stats_df['% Total Docs'] = round(100 * topic_stats_df['# of Docs'] / sum(topic_stats_df['# of Docs']), 2)\n",
    "topic_stats_df['Topic Desc'] = topics_df['Topic Desc']\n",
    "topic_stats_df.sort_values('% Total Docs', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_posts = corpus_topic_df.groupby('Dominant Topic') \\\n",
    ".apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], ascending=False).iloc[0]))\n",
    "relevant_posts.sort_values('Contribution %', ascending = False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
